# ðŸ”— Corpus AI Companion - Pipeline Architecture

*Bash-style piping for AI components: `vision | analysis | speech | audio`*

---

## ðŸ—ï¸ **Core Architecture Philosophy**

Each capability is an **isolated, upgradeable API service** that follows Unix pipe principles:
- **Standard Input/Output**: Consistent data formats between services
- **Composability**: Mix and match components freely
- **Independence**: Upgrade any component without breaking others
- **Scalability**: Run components on different machines/processes

---

## ðŸŒŠ **Data Flow Patterns**

### **Basic Pipeline: Vision â†’ Speech**
```
Camera â†’ Vision API â†’ Gemini Analysis â†’ Description â†’ Speech API â†’ Hume TTS â†’ Audio Output
```

### **Enhanced Pipeline: Multi-Modal Processing**
```
Camera â”€â”
        â”œâ”€â†’ Orchestrator â”€â†’ Context Engine â”€â†’ Speech API â”€â†’ Audio
Audio â”€â”€â”˜                                   â†‘
                                            â”‚
                                         Emotional
                                         Intelligence
```

### **WebSocket Streaming Pipeline**
```
Continuous:  Vision Stream â”€â”€â†’ Analysis Stream â”€â”€â†’ Speech Stream â”€â”€â†’ Audio Stream
Control:     REST APIs     â”€â”€â†’ Configuration   â”€â”€â†’ Session Mgmt  â”€â”€â†’ Status
```

---

## ðŸ”§ **API Contract Standards**

### **Vision API Output**
```json
{
  "timestamp": 1692304234,
  "image": {
    "format": "base64_jpeg",
    "resolution": "1920x1080",
    "data": "base64_encoded_image..."
  },
  "analysis": {
    "description": "I can see a person sitting at a desk working on a laptop...",
    "confidence": 0.95,
    "objects": ["person", "laptop", "desk"],
    "emotions": ["focused", "calm"],
    "context": "indoor_workspace"
  },
  "metadata": {
    "processing_time_ms": 245,
    "model": "gemini-1.5-flash"
  }
}
```

### **Speech API Input**
```json
{
  "text": "I can see a person sitting at a desk working on a laptop...",
  "voice_config": {
    "voice_id": "ito",
    "emotion": "calm",
    "context": "observational",
    "speed": 1.0
  },
  "output_format": "mp3",
  "metadata": {
    "source": "vision_analysis",
    "timestamp": 1692304234
  }
}
```

### **Pipeline Control Messages**
```json
{
  "command": "start_continuous_pipeline",
  "config": {
    "vision": {
      "interval_ms": 3000,
      "quality": "high"
    },
    "speech": {
      "voice_id": "ito",
      "auto_emotion": true
    }
  },
  "pipeline_id": "vision_speech_01"
}
```

---

## ðŸŽ›ï¸ **Service Orchestration**

### **REST API Layer (Current)**
- **Control Plane**: Start/stop, configuration, status
- **Data Plane**: Single-shot processing requests
- **Synchronous**: Request â†’ Response model

### **WebSocket Layer (Next Phase)**
- **Streaming Data**: Continuous real-time processing
- **Bi-directional**: Client â†” Server communication  
- **Session-based**: Persistent connections with context

### **Orchestrator Role**
```python
class PipelineOrchestrator:
    def create_pipeline(self, components: List[str]) -> Pipeline:
        """Create a processing pipeline: vision | analysis | speech"""
        
    def start_continuous_mode(self, pipeline: Pipeline, config: Dict):
        """Start real-time streaming between components"""
        
    def pipe_data(self, source_api: str, dest_api: str, transform: Callable):
        """Pipe output from one API to input of another"""
```

---

## ðŸ”„ **Pipeline Modes**

### **Mode 1: On-Demand Processing**
```bash
# REST API equivalent of: vision | speech
curl vision/describe | curl -d @- speech/speak
```

### **Mode 2: Continuous Loop**
```python
# Periodic processing with REST APIs
while running:
    description = vision_api.describe()
    speech_api.speak(description)
    sleep(interval)
```

### **Mode 3: Real-Time Streaming** *(Future WebSocket)*
```python
# Continuous streaming pipeline
vision_stream = VisionWebSocket()
speech_stream = SpeechWebSocket()
pipeline = vision_stream | analysis_filter | speech_stream
pipeline.start()
```

---

## ðŸŽ¯ **Implementation Strategy**

### **Phase 1: REST Pipeline Integration** *(Current Focus)*
- âœ… Vision API returns descriptions
- âœ… Speech API accepts text input
- ðŸ”„ Orchestrator pipes visionâ†’speech
- ðŸ”„ Continuous loop with configurable interval

### **Phase 2: WebSocket Foundation**
- ðŸš€ WebSocket endpoints for each service
- ðŸš€ Real-time data streaming
- ðŸš€ Session management and reconnection
- ðŸš€ Backpressure and flow control

### **Phase 3: Advanced Pipelines**
- ðŸŒŸ Multi-input fusion (vision + audio)
- ðŸŒŸ Context-aware processing chains
- ðŸŒŸ Emotional state propagation
- ðŸŒŸ Dynamic pipeline reconfiguration

---

## ðŸ“¡ **Network Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vision API    â”‚    â”‚  Orchestrator   â”‚    â”‚   Speech API    â”‚
â”‚   Port 5002     â”‚â—„â”€â”€â–ºâ”‚   Port 5000     â”‚â—„â”€â”€â–ºâ”‚   Port 5001     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ Camera â†’ Gemini â”‚    â”‚ Pipeline Logic  â”‚    â”‚ Text â†’ Hume TTS â”‚
â”‚ Description â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â–º Audio Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **API Endpoints for Piping**
```
# Vision API
GET  /capture          # Single image + analysis
GET  /describe          # Description with speech output
POST /start_continuous  # Begin continuous analysis
POST /stop_continuous   # End continuous analysis

# Speech API  
POST /speak             # Text â†’ Audio
POST /speak_from_vision # Accept vision API format directly
GET  /voices            # Available voice configurations

# Orchestrator
POST /pipeline/start    # Start visionâ†’speech pipeline
POST /pipeline/stop     # Stop pipeline
GET  /pipeline/status   # Pipeline state and metrics
POST /pipeline/config   # Update pipeline settings
```

---

## ðŸ”€ **Data Transformation Layers**

### **Vision â†’ Speech Adapter**
```python
def vision_to_speech_transform(vision_output: dict) -> dict:
    """Transform vision API output to speech API input"""
    return {
        "text": vision_output["analysis"]["description"],
        "voice_config": {
            "voice_id": "ito",
            "emotion": infer_emotion(vision_output["analysis"]["emotions"]),
            "context": vision_output["analysis"]["context"]
        }
    }
```

### **Emotion Context Pipeline**
```python
def add_emotional_context(description: str, visual_context: dict) -> str:
    """Enhance description with emotional awareness"""
    emotions = visual_context.get("emotions", [])
    if "peaceful" in emotions:
        return f"In this serene moment, {description}"
    elif "busy" in emotions:  
        return f"I notice the bustling activity where {description}"
    return description
```

---

## ðŸŽ® **Control Interface**

### **Pipeline Commands**
```python
# Start continuous visionâ†’speech with 5-second intervals
pipeline.start("vision|speech", interval=5000, voice="ito")

# Add emotion detection to the pipeline
pipeline.add_filter("emotion_enhancer", after="vision")

# Switch voice dynamically based on content
pipeline.add_conditional("voice_switcher", 
    condition=lambda data: "exciting" in data.emotions,
    action=lambda: switch_voice("aiden"))
```

### **Status Monitoring**
```json
{
  "pipeline_id": "vision_speech_01",
  "status": "running",
  "components": {
    "vision": {"status": "active", "fps": 0.33, "last_update": 1692304567},
    "speech": {"status": "active", "queue_length": 0, "last_spoken": 1692304565}
  },
  "metrics": {
    "uptime": 3600,
    "descriptions_generated": 1200,
    "audio_clips_played": 1198,
    "avg_processing_time_ms": 2847
  }
}
```

---

## ðŸš€ **Next Steps: Vision Integration**

1. **Create Pipeline Controller** in orchestrator
2. **Add visionâ†’speech endpoints** for direct piping
3. **Implement continuous loop** with configurable intervals
4. **Test emotional speech** with vision context
5. **Prepare WebSocket foundation** for real-time streaming

**Goal**: `curl orchestrator/pipeline/start` â†’ Your Pi continuously sees, understands, and speaks about its world with emotional intelligence! ðŸ¤–âœ¨

---

*This architecture enables the dream: "Your AI companion that sees your world and speaks about it with genuine understanding and emotion."*